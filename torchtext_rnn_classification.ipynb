{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将data.xlsx划分为训练集和测试集，文件里面已经有train.tsv和test.tsv，可以不用运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('data/data.xlsx',encoding='utf-8')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.iloc[:8000].to_csv('data/train.tsv',sep='\\t',index=False,encoding='utf-8')\n",
    "data.iloc[8000:].to_csv('data/test.tsv',sep='\\t',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入包和定义Field   include_lengths参数表示是否返回每个sequence的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "def tokenizer(x):\n",
    "    return [y for y in x]\n",
    "TEXT = data.Field(tokenize = tokenizer)  ## include_lengths = True\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将tsv格式转换成TabularDataset类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = data.TabularDataset('data/train.tsv',format='tsv',skip_header=True,\n",
    "        fields=[('sentence', TEXT),('label', LABEL)])\n",
    "test_data = data.TabularDataset('data/test.tsv', format='tsv',skip_header=True,\n",
    "        fields=[('sentence', TEXT),('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x000001EC5F070BE0>\n",
      "dict_keys(['sentence', 'label'])\n",
      "['就', '是', '一', '个', '商', '业', '区', '，', '不', '能', '算', '旅', '游', '景', '点', '吧', '。']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_data[6])\n",
    "print(train_data[6].__dict__.keys())\n",
    "print(train_data[6].sentence)\n",
    "print(train_data[6].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BucketIterator准备可迭代的数据，每个epoch里面接受一个batch_size的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator,test_iterator = data.BucketIterator.splits(\n",
    "    (train_data,test_data), batch_size = BATCH_SIZE,device = DEVICE,sort_key=lambda x: len(x.sentence),sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数和LSTM模型，我们取LSTM模型输出的前向最后一个hidden和后向最后一个hidden做拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim,n_layers,bidirectional,dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx = 1)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))   ##[sent_len,batch_size,emb_dim]\n",
    "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)   ### 是否需要截断padding部分\n",
    "        packed_output,(hidden,cell) = self.rnn(embedded)\n",
    "#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)  ##[sent_len, batch size, hid dim * num directions]\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        ### hidden[-2,:,:]  表示倒数第二层（第一层）后向传播最后一个time step的输出\n",
    "        ###  hidden[-2,:,:]  表示倒数第一层（第二层）后向传播最后一个time step的输出\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim,hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)   ## shape = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)  #output = [sent len, batch size, hid dim]  #hidden = [1, batch size, hid dim]\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化器和损失函数以及评估指标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)\n",
    "\n",
    "###分类准确率\n",
    "def accuracy(preds, y):\n",
    "    pred = torch.max(preds,-1)[1]\n",
    "    correct = (pred == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练过程和评估过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.sentence).squeeze(1)\n",
    "#         print(predictions,batch.label)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.sentence).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.421 | Train Acc: 80.14%\n",
      "\t Val. Loss: 0.471 |  Val. Acc: 83.44%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.300 | Train Acc: 87.31%\n",
      "\t Val. Loss: 0.310 |  Val. Acc: 88.28%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.259 | Train Acc: 89.64%\n",
      "\t Val. Loss: 0.218 |  Val. Acc: 91.89%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.222 | Train Acc: 90.74%\n",
      "\t Val. Loss: 0.231 |  Val. Acc: 90.50%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.199 | Train Acc: 91.92%\n",
      "\t Val. Loss: 0.242 |  Val. Acc: 88.54%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出双向lstm的效果和fasttext的效果是差不多的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
